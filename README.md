# CS431 Project: Image Generation with Adapters

This repository contains the source code, training scripts, and resources for the CS431 project. The project explores and implements **IP-Adapter**, **T2I-Adapter**, and **UniControlNet** for controlled image generation.

## ðŸ“‚ Repository Structure

*   `IPAdapter/`: Source code and scripts for IP-Adapter.
*   `UniControlNet/`: Source code and scripts for UniControlNet.
*   `So sÃ¡nh Ä‘á»‹nh tÃ­nh/`: Qualitative comparison results.
*   `áº¢nh demo/`: Demo images generated by the models.
*   `Demo.ipynb`: Jupyter Notebook for demonstrating the project results.
*   `BÃ¡o cÃ¡o Ä‘á»“ Ã¡n CS431.pdf`: Final project report.
*   `CS431_Project_slide.pdf`: Presentation slides.

## ðŸ”— Model & Resources

Pre-trained models, datasets, and checkpoints can be found at the following link:
ðŸ‘‰ [**Google Drive Folder**](https://drive.google.com/drive/folders/1Eou2X4FloFdXRUU4SOOLSx4tQ6FsF6CF)

## ðŸš€ Usage Instructions

Below are the detailed instructions for training, evaluating, and running the models.

### 1. IP-Adapter

#### Training
To train the IP-Adapter, use the `tutorial_train.py` script.

**Command:**
```bash
export CUDA_VISIBLE_DEVICES=1,2

accelerate launch --num_processes 2 --num_machines 1 --mixed_precision fp16 \
IPAdapter/tutorial_train.py \
--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 \
--image_encoder_path=openai/clip-vit-base-patch32 \
--webdataset_urls "https://huggingface.co/datasets/pixparse/cc3m-wds/resolve/main/cc3m-train-{0000..0575}.tar" \
--resolution=512 \
--train_batch_size=4 \
--steps_per_epoch=363750 \
--dataloader_num_workers=4 \
--learning_rate=1e-4 \
--weight_decay=0.01 \
--output_dir=IPAdapter/outputs/cc3m_v2 \
--save_steps=10000 \
--num_train_epochs=1 \
--wds_shuffle=4096 \
--wds_prefetch=3
```

#### Parameter Calculation
Scripts to calculate parameters for IP-Adapter and LoRA.

**IP-Adapter:**
```bash
accelerate launch --num_processes 1 --num_machines 1 --mixed_precision fp16 \
IPAdapter/count_params.py \
--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 \
--image_encoder_path=openai/clip-vit-base-patch32 \
--webdataset_urls "https://huggingface.co/datasets/pixparse/cc3m-wds/resolve/main/cc3m-train-{0000..0575}.tar" \
--resolution=512 \
--train_batch_size=4 \
--output_dir=IPAdapter/outputs/cc3m_v2 \
... (additional args as needed)
```

**LoRA:**
```bash
accelerate launch --num_processes 1 --num_machines 1 --mixed_precision fp16 \
IPAdapter/count_params_lora.py \
--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 \
--image_encoder_path=openai/clip-vit-base-patch32 \
--webdataset_urls "https://huggingface.co/datasets/pixparse/cc3m-wds/resolve/main/cc3m-train-{0000..0575}.tar" \
--resolution=512 \
--train_batch_size=4 \
--output_dir=IPAdapter/outputs/cc3m_v2 \
...
```

### 2. T2I-Adapter

#### Training
Training script for T2I-Adapter style transfer.

**Command:**
```bash
export CUDA_VISIBLE_DEVICES=1,2

accelerate launch --num_processes 2 --num_machines 1 --mixed_precision fp16 \
train_style.py \
--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 \
--image_encoder_path=openai/clip-vit-base-patch32 \
--webdataset_urls "https://huggingface.co/datasets/pixparse/cc3m-wds/resolve/main/cc3m-train-{0000..0575}.tar" \
--resolution=512 \
--train_batch_size=4 \
--steps_per_epoch=363750 \
--dataloader_num_workers=4 \
--learning_rate=1e-4 \
--weight_decay=0.01 \
--output_dir=T2I-Adapter/outputs/cc3m_v2 \
--save_steps=1000 \
--num_train_epochs=1 \
--wds_shuffle=4096 \
--wds_prefetch=3
```

#### Debugging
To run in debug mode with detailed logs:
```bash
TORCH_DISTRIBUTED_DEBUG=DETAIL NCCL_DEBUG=WARN \
accelerate launch --num_processes 2 --num_machines 1 --mixed_precision fp16 \
train_style.py \
... (same args as above) \
2>&1 | tee train_debug.log
```

### 3. UniControlNet

#### Training
Training script for UniControlNet global adapter.

**Command:**
```bash
TORCH_DISTRIBUTED_DEBUG=DETAIL NCCL_DEBUG=WARN \
accelerate launch --num_processes 2 --num_machines 1 --mixed_precision fp16 \
UniControlNet/src/train/train_global_adapter_diffusers.py \
--pretrained-model-name-or-path runwayml/stable-diffusion-v1-5 \
--train-shards "https://huggingface.co/datasets/pixparse/cc3m-wds/resolve/main/cc3m-train-{0000..0575}.tar" \
--train-batch-size 4 \
--num-workers 4 \
--drop-txt-prob 0.5 \
--resolution 512 \
--learning-rate 1e-4 \
--mixed-precision fp16 \
--weight_decay=0.01 \
--max-train-steps 363750 \
--drop-global-prob 0.5 2>&1 | tee train_debug.log
```

#### Parameter Calculation
Calculate parameters for UniControlNet.

**Command:**
```bash
export CUDA_VISIBLE_DEVICES=3

TORCH_DISTRIBUTED_DEBUG=DETAIL NCCL_DEBUG=WARN \
accelerate launch --num_processes 1 --num_machines 1 --mixed_precision fp16 \
UniControlNet/src/train/compute_para.py \
--pretrained-model-name-or-path runwayml/stable-diffusion-v1-5 \
--train-shards "https://huggingface.co/datasets/pixparse/cc3m-wds/resolve/main/cc3m-train-{0000..0575}.tar" \
--train-batch-size 4 \
--num-workers 4 \
--drop-txt-prob 0.5 \
--resolution 512 \
--learning-rate 1e-4 \
--mixed-precision fp16 \
--weight_decay=0.01 \
--max-train-steps 363750 \
--drop-global-prob 0.5 2>&1 | tee train_debug_calc.log
```

### 4. Evaluation & Metrics

#### CLIP Score Calculation
Calculate CLIP scores to evaluate image generation quality.

```bash
export CUDA_VISIBLE_DEVICES=0,3
accelerate launch --num_processes 2 --num_machines 1 --mixed_precision fp16 \
python compute_clip_scores.py \
--sd_path "runwayml/stable-diffusion-v1-5" \
--image_encoder_path "openai/clip-vit-base-patch32" \
--ip_ckpt IPAdapter/models/ip-adapter_step160000.bin \
--cc3m_shards "IPAdapter/dataset/cc3m-validation/cc3m-validation-{0000..0006}.tar" \
--num_samples_per_image 4 \
--num_inference_steps 50 \
--guidance_scale 7.5 \
--lambda_img 1.0 \
--metrics_out result_clip_10.json \
--max_images 10 2>&1 | tee train_debug.log
```

#### LoRA Evaluation
Evaluate IP-Adapter with LoRA.

```bash
python eval_ip_adapter_lora_clip.py \
--sd_path runwayml/stable-diffusion-v1-5 \
--image_encoder_path "openai/clip-vit-base-patch32" \
--image_proj_path outputs_lo/cc3m_v2/image_proj_step200000.bin \
--ip_lora_path outputs_lo/cc3m_v2/ip_lora_step200000.safetensors \
--cc3m_shards "IPAdapter/dataset/cc3m-validation/cc3m-validation-{0000..0006}.tar" \
--max_images 1000 \
--metrics_out results_clip_lora.json 2>&1 | tee train_debug.log
```

#### LoRA + MLP Evaluation
Evaluate IP-Adapter with LoRA and MLP.

```bash
python eval_ip_adapter_lora_mlp.py \
--sd_path runwayml/stable-diffusion-v1-5 \
--image_encoder_path "openai/clip-vit-base-patch32" \
--image_proj_path outputs/cc3m_v2/image_proj_step250000.bin \
--ip_lora_path outputs/cc3m_v2/ip_lora_step250000.safetensors \
--cc3m_shards "IPAdapter/dataset/cc3m-validation/cc3m-validation-{0000..0006}.tar" \
--max_images 10 \
--metrics_out results_clip_lora_mlp.json 2>&1 | tee train_debug.log
```

#### General Parameter Counting
Scripts to count parameters from checkpoints.

```bash
python count_params_ckpt.py \
--ip_lora_path outputs_lo/cc3m_v2/ip_lora_step200000.safetensors \
--image_proj_path outputs_lo/cc3m_v2/image_proj_step200000.bin

python count_params_ckpt.py \
--ckpt outputs/cc3m_v2/ip_adapter_step250000.pt

python compute_params.py \
--image-proj outputs_lo/cc3m_v2/image_proj_step110000.bin \
--ip-adapter outputs_lo/cc3m_v2/ip_adapter_step110000.bin \
--lora outputs_lo/cc3m_v2/ip_lora_step110000.safetensors
```
